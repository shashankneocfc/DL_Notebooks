{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['He', 'is'], ['He', 'the'], ['is', 'He'], ['is', 'the'], ['is', 'king'], ['the', 'He'], ['the', 'is'], ['the', 'king'], ['the', '.'], ['king', 'is'], ['king', 'the'], ['king', '.'], ['.', 'the'], ['.', 'king'], ['', 'The'], ['', 'king'], ['The', ''], ['The', 'king'], ['The', 'is'], ['king', ''], ['king', 'The'], ['king', 'is'], ['king', 'royal'], ['is', 'The'], ['is', 'king'], ['is', 'royal'], ['is', '.'], ['royal', 'king'], ['royal', 'is'], ['royal', '.'], ['royal', ''], ['.', 'is'], ['.', 'royal'], ['.', ''], ['', 'royal'], ['', '.'], ['', 'She'], ['', 'is'], ['She', ''], ['She', 'is'], ['She', 'the'], ['is', ''], ['is', 'She'], ['is', 'the'], ['is', 'royal'], ['the', 'She'], ['the', 'is'], ['the', 'royal'], ['the', ''], ['royal', 'is'], ['royal', 'the'], ['royal', ''], ['royal', 'queen'], ['', 'the'], ['', 'royal'], ['', 'queen'], ['', ''], ['queen', 'royal'], ['queen', ''], ['queen', ''], ['', ''], ['', 'queen']]\n"
     ]
    }
   ],
   "source": [
    "sentences = ['He is the king .',' The king is royal . ', ' She is the royal  queen ']\n",
    "# convert to lower case\n",
    "words = set((\" \".join(sentences)).split())\n",
    "vocab_size = len(words)\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n",
    "    \n",
    "word_window = 2\n",
    "train_data = []\n",
    "for sentence in sentences:\n",
    "    sentence_words = sentence.split(\" \")\n",
    "    length = len(sentence_words)\n",
    "    for word_idx, source_word in enumerate(sentence_words):\n",
    "        neighbor_words = sentence_words[max(0, word_idx - word_window):word_idx]  + sentence_words[word_idx + 1: min(length, word_idx + word_window + 1)]\n",
    "        for target_word in neighbor_words:\n",
    "            train_data.append([source_word, target_word])\n",
    "print(\"Vocab size = %d\"%vocab_size)\n",
    "print(train_data)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3a51d4ad8c71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert_to_onehot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert_to_onehot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "#Build training data\n",
    "def convert_to_onehot(one_index, vocab_size):\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[one_index] = 1.0\n",
    "    return onehot\n",
    "\n",
    "train_x, train_y = [], []\n",
    "for pair in train_data:\n",
    "    train_x.append(convert_to_onehot(word2int[pair[0]], vocab_size))\n",
    "    train_y.append(convert_to_onehot(word2int[pair[1]], vocab_size))\n",
    "train_x = np.asarray(train_x)\n",
    "train_y = np.asarray(train_y)\n",
    "print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 1\n",
    "        self.input_word_embed_size = 100\n",
    "        \n",
    "    def forard(self):\n",
    "        input_word_embeddings = tf.Variable(tf.random_uniform([self.vocab_size, self.input_word_embed_size], -1.0, 1.0))\n",
    "        output_word_embedding = tf.Variable(tf.zeros([self.vocab_size, self.input_word_embed_size], -1.0, 1.0))\n",
    "\n",
    "        # Global step: scalar, i.e., shape [].\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\")\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
